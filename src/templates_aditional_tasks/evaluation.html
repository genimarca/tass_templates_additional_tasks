<!DOCTYPE html>
<html>

<head lang="en">
  <title>TASS 2018 @SEPLN - Task 3</title>
  <meta charset="utf-8">
  <meta name="description" content="Workshop TASS 2018">
  <meta name="keywords" content="">

  <link rel="stylesheet" type="text/css" href="./css/tass_2017.css">
  <link rel="stylesheet" type="text/css" href="./css/custom.css">
  <link rel="stylesheet" type="text/css" href="./css/font-awesome.min.css" media="screen">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inconsolata|Roboto+Mono">

  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>
</head>

<body>
  <div class="page-wrapper">
    <header class="header">
      <div class="row">
        <div class="col-3 header_logo">
          <a class="logo" href="./">
            "TASS"
          </a>
        </div>
        <div class="col-9 header_title_wrapper">
          <h1 class="head_title">TASS-2018-Task 3. eHealth Knowledge Discovery</h1>
        </div>
      </div>
    </header>
    <main>
      <nav class="main_menu">
        <div class="row main_menu_wrapper">
          <div class="col-12">
            <ul>
              <li>
                <a href="index.html">Home</a>
              </li>
              <li>
                <a href="taskA.html">Subtask A</a>
              </li>
              <li>
                <a href="taskB.html">Subtask B</a>
              </li>
              <li>
                <a href="taskC.html">Subtask C</a>
              </li>
              <li>
                <a href="corpora.html">Corpora</a>
              </li>
              <li>
                <a href="evaluation.html">Evaluation</a>
              </li>
              <li>
                <a href="index.html#important_dates">Important dates</a>
              </li>
              <li>
                <a href="http://www.sepln.org/workshops/tass/2018">TASS-2018</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
      <div class="main_content_wrapper">
        <section>
          <div class="row">
            <div class="col-12">
              <h2 class="c29" id="evaluation">
                <span class="c28 c7">Overall evaluation </span>
              </h2>
              <p class="c52">
                <span class="c51">There </span>
                <span class="c3">will be three evaluation scenarios:</span>
              </p>
              <h3 class="c32" id="h.3rdcrjn">
                <span>Scenario 1: </span>
                <span class="c44">Only plain text is given (Subtasks A, B, C).</span>
              </h3>
              <p class="c34">
                <span>In this first scenario, the participants will perform the three subtasks consecutively and provide the corresponding
                  development output files. The only input provided are plain text files </span>
                <span class="c35">input_&lt;topic&gt;.txt </span>
                <span class="c3">for a particular list of topics that were not released with the training data.</span>
              </p>
              <p class="c34">
                <span>Systems will be ranked according to an aggregated </span>
                <span class="c7">F1</span>
                <span>metric computed on the three tasks, by considering </span>
                <span class="c7">precision</span>
                <span>and </span>
                <span class="c7">recall</span>
                <span class="c3">as follows:</span>
              </p>
              <p class="c30">
                $$ precision(A,B,C) = \frac{Correct(A) + Correct(B) + Correct(C) + \frac{1}{2} Partial(A) + \frac{1}{2} Partial(C)}{Spurious(A)
                + Partial(A) + Correct(A) + Correct(B) + Incorrect(B) + Spurious(C) + Correct(C) + Partial(C)} $$
              </p>
              <p class="c30">
                $$ recall(A,B,C) = \frac{Correct(A) + Correct(B) + Correct(C) + \frac{1}{2} Partial(A) + \frac{1}{2} Partial(C)}{Missing(A)
                + Partial(A) + Correct(A) + Correct(B) + Incorrect(B) + Missing(C) + Correct(C) + Partial(C)} $$
              </p>
              <p class="c30">
                $$ F_1(A,B,C) = 2 \cdot \frac{precision \cdot recall}{precision + recall} $$
              </p>

              <h3 class="c23" id="h.26in1rg">
                <span>Scenario 2: </span>
                <span class="c44">Plain text and manually annotated key phrase boundaries are given (Subtasks B, C).</span>
              </h3>
              <p class="c2">
                <span>In this second scenario participants will perform tasks B and C sequentially, and provide the corresponding
                  output files. As input, they receive both plain text files (</span>
                <span class="c35">input_&lt;topic&gt;.txt</span>
                <span>), and the corresponding gold files for the task A (</span>
                <span class="c35">output_A_&lt;topic&gt;.txt</span>
                <span>). The purpose of this scenario is to evaluate the quality of tasks B and C independently from task A. As
                  in the previous scenario, an aggregated </span>
                <span class="c7">F1</span>
                <span>metric is reported, based on the following </span>
                <span class="c7">precision</span>
                <span>and </span>
                <span class="c7">recall</span>
                <span class="c3">:</span>
              </p>
              <p class="c30">
                $$ precision(B,C) = \frac{Correct(B) + Correct(C) + \frac{1}{2} Partial(C)}{Correct(B) + Incorrect(B) + Spurious(C) + Correct(C)
                + Partial(C)} $$
              </p>
              <p class="c30">
                $$ recall(B,C) = \frac{Correct(B) + Correct(C) + \frac{1}{2} Partial(C)}{Correct(B) + Incorrect(B) + Missing(C) + Correct(C)
                + Partial(C)} $$
              </p>
              <p class="c30">
                $$ F_1(B,C) = 2 \cdot \frac{precision \cdot recall}{precision + recall} $$
              </p>
              <h3 class="c23" id="h.lnxbz9">
                <span>Scenario 3: </span>
                <span class="c44">Plain text with manually annotated key phrases and their types are given (Subtask C).</span>
              </h3>

              <p class="c34">
                <span>In this scenario both the gold outputs for task A and task B are provided, and the participants must only
                  perform the process to obtain task C output files. The purpose of this scenario is to evaluate only the
                  quality of task C independently of the complexity of task A and B. As before, an aggregated F1 metric is
                  reported, based on the following </span>
                <span class="c7">precision</span>
                <span>and </span>
                <span class="c7">recall</span>
                <span class="c3">:</span>
              </p>
              <p class="c30">
                $$ precision(C) = \frac{Correct(C) + \frac{1}{2} Partial(C)}{Spurious(C) + Correct(C) + Partial(C)} $$
              </p>
              <p class="c30">
                $$ recall(C) = \frac{Correct(C) + \frac{1}{2} Partial(C)}{Missing(C) + Correct(C) + Partial(C)} $$
              </p>
              <p class="c30">
                $$ F_1(C) = 2 \cdot \frac{precision \cdot recall}{precision + recall} $$
              </p>

              <h3 class="c23" id="h.lnxbz9">
                <span>Final Score: </span>
                <span class="c44">A macro-average of all three scenarios is given as the final score:</span>
              </h3>

              <p>
                $$ F_{final} = \frac{1}{3} F_1(A,B,C) + \frac{1}{3} F_1(B,C) + \frac{1}{3} F_1(C) $$
              </p>

              <br>
              <br>

              <h2>Baseline implementation</h2>

              <p>
                A baseline implementation is provided in
                <a href="https://github.com/TASS18-Task3/data/tree/master/baseline">https://github.com/TASS18-Task3/data/tree/master/baseline</a>. This implementation simply counts the number
                of occurrences of all concepts, actions and relations, and uses these statistics to match the exact same
                occurrences. Hence, it can be used as a minimal baseline of the expected score in each evaluation scenario.
              </p>

              <p>
                Feel free to use and modify this baseline implementation, both for testing the submission process in Codalab and as a template
                for developing your own implementation if necessary.
              </p>

              <h2>Evaluation scripts</h2>

              <p>
                There are three differents ways of obtaining an evaluation. The first option is to use the script
                <a href="https://github.com/TASS18-Task3/data/blob/master/score_training.py">score_training.py</a>
                which provides a detailed evaluation including a report of each of the mistakes (i.e., spurious or missing items) for each
                of the training files. This script will be mostly useful for the development of the models.
              </p>

              <p>
                The second option is to use the script
                <a href="https://github.com/TASS18-Task3/data/blob/master/score_test.py">score_test.py</a> is
                which provides a summarized report of the evaluations metrics described in this page.
                The script reports precission, recall and F1 for each scenario, and the macro average of all the F1 metrics.
              </p>

              <p>
                The final option is to submit the results to Codalab, either in the training or the testing phase.
                In this submission, the results will be exactly the same as those output by the <i>score_test.py</i>
                script. The only difference with option 2 is that during the testing phase, participants will
                not have access to the testing gold outputs, hence, the only way to obtain a testing score is to submit
                to Codalab.
              </p>

              <p>
                More information can be found in
                <a href="https://github.com/TASS18-Task3/data/blob/master/Readme.md">the Readme file</a>.
              </p>

            </div>
          </div>

        </section>
      </div>
    </main>
    <footer>
    </footer>
  </div>
</body>

</html>